#!/usr/bin/env python
"""Runner script for benchmarking {PROJECT_TITLE} nodes."""

import argparse
import json
import logging
import sys
from pathlib import Path

from dotenv import load_dotenv

from {PACKAGE_NAME}.blockchain import update_json_metadata
from {PACKAGE_NAME}.database import store_benchmark_data_in_db
from {PACKAGE_NAME}.main import run_benchmarks

# Constants for this project
RESULTS_FILE = "{PROJECT_NAME}_results.json"


def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description="Benchmark {PROJECT_TITLE} nodes")
    parser.add_argument(
        "--seconds",
        "-s",
        type=int,
        default=30,
        help="Time limit for each benchmark in seconds (default: 30)",
    )
    parser.add_argument(
        "--no-threading", action="store_true", help="Disable threading for benchmarks"
    )
    parser.add_argument(
        "--retries",
        "-r",
        type=int,
        default=3,
        help="Number of connection retries (default: 3)",
    )
    parser.add_argument(
        "--call-retries",
        "-c",
        type=int,
        default=3,
        help="Number of API call retries (default: 3)",
    )
    parser.add_argument(
        "--timeout",
        "-t",
        type=int,
        default=30,
        help="Connection timeout in seconds (default: 30)",
    )
    parser.add_argument(
        "--output",
        "-o",
        type=str,
        default=RESULTS_FILE,
        help=f"Output file for benchmark results (JSON format) (default: {RESULTS_FILE})",
    )
    parser.add_argument(
        "--report-file",
        "-f",
        type=str,
        help="Existing report file to use instead of running benchmarks",
    )
    parser.add_argument(
        "--no-db", action="store_true", help="Do not store results in database"
    )
    parser.add_argument(
        "--update-metadata",
        "-u",
        action="store_true",
        help="Update account JSON metadata with benchmark results",
    )
    parser.add_argument(
        "--verbose", "-v", action="store_true", help="Enable verbose logging"
    )
    # Add any project-specific arguments here
    {PROJECT_SPECIFIC_ARGS}
    
    return parser.parse_args()


def main():
    """Main entry point for benchmarking script."""
    # Load environment variables from .env file
    load_dotenv()

    args = parse_args()

    # Configure logging
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=log_level, format="%(asctime)s - %(levelname)s - %(message)s"
    )

    report = None
    output_path = None

    # If a report file is provided, load it instead of running benchmarks
    if args.report_file:
        try:
            report_path = Path(args.report_file)
            if not report_path.exists():
                logging.error(f"Report file not found: {report_path}")
                return 1

            logging.info(f"Loading report from {report_path}")
            with open(report_path, "r") as f:
                report = json.load(f)

            # If --output is specified, use it as the output path
            if args.output:
                output_path = Path(args.output)
                if report_path != output_path:  # Only save if it's a different file
                    with open(output_path, "w") as f:
                        json.dump(report, f, indent=2)
                    logging.info(f"Copied report to {output_path}")
                else:
                    output_path = report_path
            else:
                output_path = report_path
        except Exception as e:
            logging.error(f"Failed to load report file: {e}")
            return 1

    # Run benchmarks if no report file was provided or if --update-metadata was not specified
    if report is None and (not args.update_metadata or not args.report_file):
        logging.info("Running benchmarks...")
        # Project specific arguments for run_benchmarks
        {PROJECT_SPECIFIC_BENCHMARK_ARGS}
        report = run_benchmarks(
            seconds=args.seconds,
            threading=not args.no_threading,
            num_retries=args.retries,
            num_retries_call=args.call_retries,
            timeout=args.timeout,
            {ADDITIONAL_BENCHMARK_ARGS}
        )

        # Store results in database if requested
        if not args.no_db:
            store_benchmark_data_in_db(report)
            logging.info("Results stored in database.")

        # Output results to file if requested
        if args.output:
            output_path = Path(args.output)
            with open(output_path, "w") as f:
                json.dump(report, f, indent=2)
            logging.info(f"Results written to {output_path}")
    elif report is None:
        logging.error("No report file provided and no benchmarks run.")
        return 1

    # Update the JSON metadata if requested
    if args.update_metadata:
        try:
            logging.info("Updating JSON metadata...")
            tx = update_json_metadata(report)
            logging.info(f"Updated JSON metadata: {tx}")
        except Exception as e:
            logging.error(f"Failed to update JSON metadata: {e}")
            return 1

    # Print summary to console
    print("\nBenchmark Summary:")
    print(f"Tested {len(report['nodes'])} working nodes and {len(report['failing_nodes'])} failing nodes")

    # Project-specific summary printing
    {PROJECT_SPECIFIC_SUMMARY}

    return 0


if __name__ == "__main__":
    sys.exit(main())
